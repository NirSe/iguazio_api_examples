{"paragraphs":[{"title":"Overview","text":"%md\n\nThis note contains code examples for performing common tasks to help you get started with the Iguazio Continous Data Platform (\"the platform\").\nFollow the tutorial by running the note paragraphs in order of appearance.\n\n> **Tip:** You can also browse the files and directories that you write to the \"bigdata\" container in this tutorial from the platform dashboard: in the side navigation menu, select **Data**, and then select the **bigdata** container from the table. On the container data page, select the **Browse** tab, and then use the side directory-navigation tree to browse the directories. Selecting a file or directory in the browse table displays its metadata.\n\nFor more information about the platform &mdash; including tutorial, demos, and code examples &mdash; read [the platform documentation](https://www.iguazio.com/docs/).\nFor technical questions and assistance, don't hestistate to contact support@iguazio.com.","user":"anonymous","dateUpdated":"2018-09-05T10:37:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This note contains code examples for performing common tasks to help you get started with the Iguazio Continous Data Platform (&ldquo;the platform&rdquo;).<br/>Follow the tutorial by running the note paragraphs in order of appearance.</p>\n<blockquote>\n  <p><strong>Tip:</strong> You can also browse the files and directories that you write to the &ldquo;bigdata&rdquo; container in this tutorial from the platform dashboard: in the side navigation menu, select <strong>Data</strong>, and then select the <strong>bigdata</strong> container from the table. On the container data page, select the <strong>Browse</strong> tab, and then use the side directory-navigation tree to browse the directories. Selecting a file or directory in the browse table displays its metadata.</p>\n</blockquote>\n<p>For more information about the platform &mdash; including tutorial, demos, and code examples &mdash; read <a href=\"https://www.iguazio.com/docs/\">the platform documentation</a>.<br/>For technical questions and assistance, don&rsquo;t hestistate to contact <a href=\"mailto:&#115;&#x75;&#112;&#x70;&#111;&#114;&#116;&#x40;&#x69;&#x67;&#117;&#97;&#122;&#x69;&#x6f;&#46;&#99;o&#x6d;\">&#115;&#x75;&#112;&#x70;&#111;&#114;&#116;&#x40;&#x69;&#x67;&#117;&#97;&#122;&#x69;&#x6f;&#46;&#99;o&#x6d;</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1536134832799_-1177022817","id":"20180904-232354_602158247","dateCreated":"2018-09-05T08:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:285","dateFinished":"2018-09-05T10:37:54+0000","dateStarted":"2018-09-05T10:37:54+0000"},{"title":"Step 1: Ingest a sample CSV file into the platform","text":"%md\n\nUse `curl` to download the sample **bank.csv** file that is used in Zeppelin's getting-started tutorial from [the Amazon S3 website](https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv) to the **/tmp** directory in the local file system of your platform. Then, use `hadoop fs` to move the file from the local file system to a new **zeppelin_getting_started_example** directory in the platform's \"bigdata\" data container.  \n","user":"anonymous","dateUpdated":"2018-09-05T10:02:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Use <code>curl</code> to download the sample <strong>bank.csv</strong> file that is used in Zeppelin&rsquo;s getting-started tutorial from <a href=\"https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\">the Amazon S3 website</a> to the <strong>/tmp</strong> directory in the local file system of your platform. Then, use <code>hadoop fs</code> to move the file from the local file system to a new <strong>zeppelin_getting_started_example</strong> directory in the platform&rsquo;s &ldquo;bigdata&rdquo; data container.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1536134832808_-531086573","id":"20180904-181320_2082294231","dateCreated":"2018-09-05T08:07:12+0000","dateStarted":"2018-09-05T10:02:32+0000","dateFinished":"2018-09-05T10:02:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:286"},{"title":"Ingest a sample CSV file into the platform","text":"%sh \n\n# Download the sample bank.csv file that is used in the basic Zeppelin Tutorial from the AWS S3 web site to the local /tmp directory\ncurl \"https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\" > /tmp/bank.csv\n\n# Create a zeppelin_getting_started_example directory in the \"bigdata\" container of your platform cluster\nhadoop fs -mkdir v3io://bigdata/zeppelin_getting_started_example\n\n# Move the CSV file from the local file-system /tmp directory to the new zeppelin_getting_started_example directory in the \"bigdata\" container\nhadoop fs -moveFromLocal /tmp/bank.csv v3io://bigdata/zeppelin_getting_started_example/\n\n# Verify that the \"bigdata\" container now has a zeppelin_getting_started_example directory with the bank.csv file\nhadoop fs -ls v3io://bigdata/zeppelin_getting_started_example/","user":"anonymous","dateUpdated":"2018-09-05T10:10:11+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"},"editorMode":"ace/mode/sh","colWidth":11,"fontSize":9,"editorHide":false,"title":false,"results":{"0":{"graph":{"mode":"table","height":114,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832808_-1509729737","id":"20170531-085610_156030449","dateCreated":"2018-09-05T08:07:12+0000","dateStarted":"2018-09-05T10:10:11+0000","dateFinished":"2018-09-05T10:10:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:287","errorMessage":""},{"title":"Step 2: Convert the sample CSV file to a NoSQL table","text":"%md\n\nRead the sample **bank.csv** file that you downloaded in Step 1 into a Spark DataFrame, and write the data in NoSQL format to a new **bank_nosql** table in the **zeppelin_getting_started_example** directory that you created in the \"bigdata\" container.\nBefore writing the CSV file to the NoSQL table, add an `\"id\"` column with unique values to the DataFrame. This column will serve as the table's primary-key attribute, which uniquely identifies the table items.\n> **Note:** To use the Iguazio Spark Connector to read and write data in the platform, set the data-source format in the call to the Spark DataFrame `format` method to the platform's custom NoSQL data source &mdash; `\"io.iguaz.v3io.spark.sql.kv\"`.\n","user":"anonymous","dateUpdated":"2018-09-05T09:58:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Read the sample <strong>bank.csv</strong> file that you downloaded in Step 1 into a Spark DataFrame, and write the data in NoSQL format to a new <strong>bank_nosql</strong> table in the <strong>zeppelin_getting_started_example</strong> directory that you created in the &ldquo;bigdata&rdquo; container.<br/>Before writing the CSV file to the NoSQL table, add an <code>&quot;id&quot;</code> column with unique values to the DataFrame. This column will serve as the table&rsquo;s primary-key attribute, which uniquely identifies the table items.</p>\n<blockquote>\n  <p><strong>Note:</strong> To use the Iguazio Spark Connector to read and write data in the platform, set the data-source format in the call to the Spark DataFrame <code>format</code> method to the platform&rsquo;s custom NoSQL data source &mdash; <code>&quot;io.iguaz.v3io.spark.sql.kv&quot;</code>.</p>\n</blockquote>\n</div>"}]},"apps":[],"jobName":"paragraph_1536134832808_589119731","id":"20180905-005723_1067875008","dateCreated":"2018-09-05T08:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:288","dateFinished":"2018-09-05T09:58:18+0000","dateStarted":"2018-09-05T09:58:18+0000"},{"title":" Convert the sample CSV file to a NoSQL table","text":"%spark\n\nimport org.apache.spark.sql.SparkSession\n\n// Read the sample bank.csv file from the zeppelin_getting_started_example \"bigdata\" container into a Spark DataFrame, and let Spark infer the schema of the CSV file\nval myDF = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").csv(\"v3io://bigdata/zeppelin_getting_started_example/bank.csv\")\n\n// Add an \"id\" column with unique auto-generated sequential values\nval nosqlDF = myDF.withColumn(\"id\", monotonically_increasing_id+1)\n\n// Show the DataFrame data\nnosqlDF.show()\n\n// Write the DataFrame data to a zeppelin_example/bank_nosql NoSQL table in the platform's \"bigdata\" container.\n// The data source is set in the format call to \"io.iguaz.v3io.spark.sql.kv\" - the platform's custom NoSQL data source.\n// The \"id\" column (attribute) is defined as the table's primary-key attribute, which uniquely identifies table items.\nnosqlDF.write.format(\"io.iguaz.v3io.spark.sql.kv\").mode(\"append\").option(\"key\", \"id\").save(\"v3io://bigdata/zeppelin_getting_started_example/bank_nosql/\")\n\n// Show the schema of of the DataFrame data\nnosqlDF.printSchema()\n\n// Create a temporay view named \"bank\" for quering the DataFrame data using Spark SQL\nnosqlDF.createOrReplaceTempView(\"bank\")\n","user":"anonymous","dateUpdated":"2018-09-05T10:18:39+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"fontSize":9,"editorHide":false,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832809_511557230","id":"20180530-134241_499121492","dateCreated":"2018-09-05T08:07:12+0000","dateStarted":"2018-09-05T10:18:39+0000","dateFinished":"2018-09-05T10:18:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289","errorMessage":""},{"title":"Step 3: Query the data with the Scala Spark interpreter","text":"%md\n\nUse the Scala Spark interpreter (`%spark`) to query the temporary `bank` view for the average bank balance for each age. ","user":"anonymous","dateUpdated":"2018-09-05T10:04:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Use the Scala Spark interpreter (<code>%spark</code>) to query the temporary <code>bank</code> view for the average bank balance for each age.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1536134832809_-1233218369","id":"20180905-020939_549284275","dateCreated":"2018-09-05T08:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:290","dateFinished":"2018-09-05T10:04:37+0000","dateStarted":"2018-09-05T10:04:37+0000"},{"title":"Query the data with the Scala Spark interpreter","text":"%spark\n\n// Use Spark SQL to query the temporary \"bank\" view \nval sqlDF = spark.sql(\"select age, round(avg(balance)) from bank group by age order by age asc\")\n\n// Show the first 10 lines of the query result\nsqlDF.show(10)","user":"anonymous","dateUpdated":"2018-09-05T10:21:01+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"fontSize":9,"editorHide":false,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832809_-1737660558","id":"20180530-134533_2077552445","dateCreated":"2018-09-05T08:07:12+0000","dateStarted":"2018-09-05T10:21:01+0000","dateFinished":"2018-09-05T10:21:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:291","errorMessage":""},{"title":"Step 4: Query the data with the SQL interpreter","text":"%md\n\nUse the SQL interpreter (`%sql`) to query the temporary `bank` view for the number customers in each age under 30 and visualize the results graphically.\n","user":"anonymous","dateUpdated":"2018-09-05T10:23:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832809_1463142946","id":"20180905-022107_2693988","dateCreated":"2018-09-05T08:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:292","dateFinished":"2018-09-05T10:23:54+0000","dateStarted":"2018-09-05T10:23:54+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Use the SQL interpreter (<code>%sql</code>) to query the temporary <code>bank</code> view for the number customers in each age under 30 and visualize the results graphically.</p>\n</div>"}]}},{"title":"Query the data with the SQL interpreter","text":"%sql\n\nselect age, count(1) value\nfrom bank \nwhere age < 30 \ngroup by age \norder by age\n","user":"anonymous","dateUpdated":"2018-09-05T10:24:01+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","colWidth":12,"fontSize":9,"editorHide":false,"title":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"multiBarChart":{"stacked":false,"rotate":{"degree":"-45"},"xLabelStatus":"default"},"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"age":"string","value":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"age","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"value","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832810_-313894463","id":"20180531-173242_263092943","dateCreated":"2018-09-05T08:07:12+0000","dateStarted":"2018-09-05T10:21:22+0000","dateFinished":"2018-09-05T10:21:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:293","errorMessage":""},{"title":"Step 5: Convert the NoSQL table to a Parquet table","text":"%md\n\nRead the **zeppelin_getting_started_example/bank_nosql** table from the \"bigdata\" container into a Spark DataFrame, and write the data in Parquet format to a new **zeppelin_getting_started_example/bank_prqt** table in the \"bigdata\" container.\n","user":"anonymous","dateUpdated":"2018-09-05T10:05:47+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832810_-1875951193","id":"20180905-022348_664626578","dateCreated":"2018-09-05T08:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:294","dateFinished":"2018-09-05T10:05:47+0000","dateStarted":"2018-09-05T10:05:47+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Read the <strong>zeppelin_getting_started_example/bank_nosql</strong> table from the &ldquo;bigdata&rdquo; container into a Spark DataFrame, and write the data in Parquet format to a new <strong>zeppelin_getting_started_example/bank_prqt</strong> table in the &ldquo;bigdata&rdquo; container.</p>\n</div>"}]}},{"title":"Convert the NoSQL table to a Parquet table","text":"%spark\n\n// Read the contents of the zeppelin_getting_started_example/bank_nosql NoSQL table in the platform's \"bigdata\" container into a Spark DataFrame\nval prqtDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(\"v3io://bigdata/zeppelin_getting_started_example/bank_nosql/\")\n\n// Write the DataFrame data in Parquet format to a zeppelin_getting_started_example/bank_prqt table in the \"bigdata\" container\nprqtDF.write.format(\"parquet\").mode(\"append\").save(\"v3io://bigdata/zeppelin_getting_started_example/bank_prqt/\")\n","user":"anonymous","dateUpdated":"2018-09-05T10:25:53+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"fontSize":9,"editorHide":false,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832810_-565413533","id":"20170530-144529_1722628679","dateCreated":"2018-09-05T08:07:12+0000","dateStarted":"2018-09-05T10:25:53+0000","dateFinished":"2018-09-05T10:26:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:295","errorMessage":""},{"title":"Step 6: Display the contents of the example container directory","text":"%md\n\nUse `hadoop fs` to list the contents of the **zeppelin_getting_started_example** directory in the platform's \"bigdata\" container.\nYou should see in this directory the **bank.csv** file and the **bank_nosql** and **bank_prqt** table directories.\n","user":"anonymous","dateUpdated":"2018-09-05T10:27:47+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832811_-850956564","id":"20180905-023325_149483854","dateCreated":"2018-09-05T08:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:296","dateFinished":"2018-09-05T10:27:47+0000","dateStarted":"2018-09-05T10:27:47+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Use <code>hadoop fs</code> to list the contents of the <strong>zeppelin_getting_started_example</strong> directory in the platform&rsquo;s &ldquo;bigdata&rdquo; container.<br/>You should see in this directory the <strong>bank.csv</strong> file and the <strong>bank_nosql</strong> and <strong>bank_prqt</strong> table directories.</p>\n</div>"}]}},{"title":"Display the contents of the example container directory","text":"%sh\n\n# List the files andd directories in the \"zeppelin_getting_started_example\" directory in the \"bigdata\" container\nhadoop fs -ls v3io://bigdata/zeppelin_getting_started_example","user":"anonymous","dateUpdated":"2018-09-05T10:29:06+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"fontSize":9,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832811_-453267570","id":"20180603-164632_1138465582","dateCreated":"2018-09-05T08:07:12+0000","dateStarted":"2018-09-05T10:28:35+0000","dateFinished":"2018-09-05T10:28:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:297","errorMessage":""},{"title":"Step 7: Cleanup","text":"%md\n\nWhen you are done, you can optionally delete the files and directories created in this tutorial.\nThe clean up is done with the `hadoop fs -rm -r` command.","user":"anonymous","dateUpdated":"2018-09-05T08:07:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>When you are done, you can optionally delete the files and directories created in this tutorial.<br/>The clean up is done with the <code>hadoop fs -rm -r</code> command.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1536134832811_-1768891323","id":"20180902-153051_349792782","dateCreated":"2018-09-05T08:07:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"title":"Cleanup","text":"%sh\n\n# Delete the zeppelin_getting_started_example directory in the \"bigdata\" container\nhadoop fs -rm -r v3io://bigdata/zeppelin_getting_started_example/\n\n# Verfiy that the \"bigdata\" container no longer has a zeppelin_getting_started_example directory\n#hadoop fs -ls v3io://bigdata/","user":"anonymous","dateUpdated":"2018-09-05T10:39:10+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536134832812_-431214856","id":"20180905-000012_84830908","dateCreated":"2018-09-05T08:07:12+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:299","dateFinished":"2018-09-05T10:39:23+0000","dateStarted":"2018-09-05T10:39:10+0000"}],"name":"Iguazio Getting Started Example","id":"2DRQX6XYB","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}